# -*- coding: utf-8 -*-
"""shahaan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K36O3LZEtODCaTLnd3k1ZSe2Rcq5xCHo
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Verify File Path
file_path = 'Fuel_cell_performance_data-Full.csv'  # Replace with the actual file path

if not os.path.exists(file_path):
    print("File not found. Please upload the file to the working directory.")
    # Creating a mock dataset for testing
    data = pd.DataFrame({
        'Feature1': np.random.rand(100),
        'Feature2': np.random.rand(100),
        'Target5': np.random.rand(100)
    })
    print("Using mock dataset for testing.")
else:
    # Step 2: Load Dataset
    data = pd.read_csv(file_path)

# Display dataset info
print("Initial Dataset Head:")
print(data.head())
print("\nInitial Dataset Info:")
print(data.info())
print("\nMissing Values Before Handling:")
print(data.isnull().sum())

# Step 3: Preprocessing
# Handle missing values (fill with median for numerical and mode for categorical)
for col in data.columns:
    if data[col].isnull().sum() > 0:
        if data[col].dtype == 'object':
            data[col].fillna(data[col].mode()[0], inplace=True)
        else:
            data[col].fillna(data[col].median(), inplace=True)

# Check for categorical features and encode them
categorical_cols = data.select_dtypes(include=['object']).columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le  # Store encoders if needed for inverse transformation

# Scale numerical features
scaler = StandardScaler()
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.drop('Target5')
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

print("\nPreprocessed Dataset Head:")
print(data.head())

# Step 4: Exploratory Data Analysis (EDA)
sns.histplot(data['Target5'], kde=True)
plt.title("Distribution of Target Variable (Target5)")
plt.xlabel("Target5")
plt.ylabel("Frequency")
plt.show()

# Step 5: Split Dataset into Features and Target
X = data.drop(columns=['Target5'])  # Features
y = data['Target5']  # Target variable

# Split into 70/30 train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"Training Set Size: {X_train.shape}")
print(f"Testing Set Size: {X_test.shape}")

# Step 6: Train Multiple Prediction Models
# Initialize models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

results = []

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    results.append({'Model': name, 'MSE': mse, 'R2': r2})

# Convert results to DataFrame
results_df = pd.DataFrame(results)
print("\nModel Performance Results:")
print(results_df)

# Step 7: Visualize Model Performance
sns.barplot(x='Model', y='R2', data=results_df)
plt.title("Model Performance Comparison (R2 Score)")
plt.ylabel("R2 Score")
plt.xlabel("Model")
plt.show()

# Step 8: Save Results for GitHub
results_df.to_csv("model_results_target5.csv", index=False)  # Save model results